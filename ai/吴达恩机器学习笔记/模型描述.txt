本课程中以下变量代表：
	m表示训练集样本数量
	x代表输入变量/特征
	y代表输出变量/预测的目标变量
	那么(x,y)则表示一个训练样本
	(x^i,y^i)表示特定的训练样本,即x的第i行 y的第i行
		？和上面的区别是什么了
		
		
	h 假设函数
		How do we represent h：
		当y是一个关于X的单变量线性函数，我们可这么表示h.
		h(x) =theta_0+theta_1*x
			_表示后面的字母是小写的意思
			这些Θ_i我把它们称为模型参数
		
		上面可称为这是一个单变量线性回归算法的预测函数	

	最小化
		模型的训练就是得出Θ_0，和Θ_1，使函数直线尽量地与这些数据点很好的拟合
		Θ_0，和Θ_1的最小化过程
			指得出Θ_0，和Θ_1的值来使这个表达式的值最小，
	那怎么最小化了？
	代价函数又是指什么了？
	
	所以我其实想要做的是对所有训练样本(对i=l到i=M的样本)进行一个求和，
	最终我们能得出了下面的求和公式：
		./代价函数数学公式.png
		对i=l到i=M的样本一个求和公式
		附：
			该公式也叫平方误差代价函数。
			平方误差代价函数是解决回归问题最常用的手段之一了。
		？
			为什么要平方
			为什么 除以2m，m是因为3个数相加 取平均值 所以要除以3 ，2为什么了
	最小化
		所以我们要尽量选择参数值，使得通过最少的训练集，我们就能合理准确地预测y的值
		让我们给出标准的定义，在线性回归中我们要解决的是Θ_0和Θ_1一个最小化问题，
		即模型参数最小化会使，h(x)和y之间的差异要小
			即 减少(h(x) - y)^2，
			
		？
			参数值都定下来了，那么就此案例讲假设函数不就定下来了，为什么还需要训练集了，任意一个x放入不就可以计算出y了
	
		
		
	我的训练集中预测值和真实值的差的平方的和的1/2M最小的Θ_0和Θ_1的值
